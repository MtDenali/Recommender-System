{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vepHX7V3KvnW"
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lk9c8YbEL_ef",
    "outputId": "450946af-39e3-43b2-cf67-9a3aae77f072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ap3SsUo1K02C",
    "outputId": "926deb3e-8265-492a-9a71-931e85ef5b17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.680866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25637\n",
      "[LightGBM] [Info] Number of data points in the train set: 723412, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.757615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25623\n",
      "[LightGBM] [Info] Number of data points in the train set: 716683, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.433278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25659\n",
      "[LightGBM] [Info] Number of data points in the train set: 719111, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.430183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25631\n",
      "[LightGBM] [Info] Number of data points in the train set: 718768, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.435572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25501\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 136\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lightgbm import LGBMRanker\n",
    "\n",
    "# Load the dataset for one fold\n",
    "def load_one_fole(data_path):\n",
    "    X_train, y_train, qid_train = load_svmlight_file(str(data_path + 'train.txt'), query_id=True)\n",
    "    X_val, y_val, qid_val = load_svmlight_file(str(data_path + 'vali.txt'), query_id=True)\n",
    "\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    _, group_train = np.unique(qid_train, return_counts=True)\n",
    "    _, group_val = np.unique(qid_val, return_counts=True)\n",
    "    return X_train, y_train, qid_train, group_train, X_val, y_val, qid_val, group_val\n",
    "\n",
    "def ndcg_single_query(y_score, y_true, k):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "# calculate NDCG score given a trained model\n",
    "def compute_ndcg_all(model, X_test, y_test, qids_test, k=10):\n",
    "    unique_qids = np.unique(qids_test)\n",
    "    ndcg_ = list()\n",
    "    for i, qid in enumerate(unique_qids):\n",
    "        y = y_test[qids_test == qid]\n",
    "\n",
    "        if np.sum(y) == 0:\n",
    "            continue\n",
    "\n",
    "        p = model.predict(X_test[qids_test == qid])\n",
    "\n",
    "        idcg = ndcg_single_query(y, y, k=k)\n",
    "        ndcg_.append(ndcg_single_query(p, y, k=k) / idcg)\n",
    "    return np.mean(ndcg_)\n",
    "\n",
    "# get importance of features\n",
    "def get_feature_importance(model, importance_type='gain'):\n",
    "  importance_df = (\n",
    "    pd.DataFrame({\n",
    "        'feature_name': model.booster_.feature_name(),\n",
    "        'importance_gain': model.booster_.feature_importance(importance_type='gain'),\n",
    "    })\n",
    "    .sort_values('importance_gain', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "  return importance_df\n",
    "\n",
    "ndcg3 = []\n",
    "ndcg5 = []\n",
    "ndcg10 = []\n",
    "top5s = []\n",
    "feature_importance = []\n",
    "# for i in range(1,2):\n",
    "for i in range(1,6):\n",
    "  path = 'drive/MyDrive/MSLR-WEB10K/Fold' + str(i) + '/'\n",
    "  X_train, y_train, qid_train, group_train, X_val, y_val, qid_val, group_val = load_one_fole(path)\n",
    "\n",
    "  gbm = LGBMRanker(objective= 'lambdarank')\n",
    "  model = gbm.fit(X_train, y_train, group=group_train)\n",
    "\n",
    "\n",
    "  ndcg3.append(compute_ndcg_all(model, X_val, y_val, qid_val, k=3))\n",
    "  ndcg5.append(compute_ndcg_all(model, X_val, y_val, qid_val, k=5))\n",
    "  ndcg10.append(compute_ndcg_all(model, X_val, y_val, qid_val, k=10))\n",
    "  top5s.append(get_feature_importance(model))\n",
    "  feature_importance.append(model.booster_.feature_importance(importance_type='gain'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDj0ffNcL0Jc",
    "outputId": "30b8d081-28db-4d24-c998-5896df4d7f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's performance (nDCG3) on test set for different folds:\n",
      " [0.46533315500113603, 0.4513238963933018, 0.4611828580288599, 0.44747518797567715, 0.46197214361690514]\n",
      "model's performance (nDCG5) on test set for different folds:\n",
      " [0.47013914781237626, 0.4601948976846903, 0.4643478855231082, 0.4562056520782107, 0.4704871221202469]\n",
      "model's performance (nDCG10) on test set for different folds:\n",
      " [0.49023661809449154, 0.4819255308954761, 0.4814498486560052, 0.4762678196678019, 0.48896202334511024]\n"
     ]
    }
   ],
   "source": [
    "print ('model\\'s performance (nDCG3) on test set for different folds:\\n', ndcg3)\n",
    "print ('model\\'s performance (nDCG5) on test set for different folds:\\n', ndcg5)\n",
    "print ('model\\'s performance (nDCG10) on test set for different folds:\\n', ndcg10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yBwMbYTQI-L"
   },
   "source": [
    "### QUESTION 15:\n",
    "- Result Analysis and Interpretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKPZ-qH6L0z3",
    "outputId": "3a619f76-a67f-453f-e098-1c3bd425ec16",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 most important features when trained on fold 0:\n",
      "   feature_name  importance_gain\n",
      "0   Column_133     23856.702951\n",
      "1     Column_7      4248.546391\n",
      "2   Column_107      4135.244450\n",
      "3    Column_54      4078.463216\n",
      "4   Column_129      3635.037024 \n",
      "\n",
      "top 5 most important features when trained on fold 1:\n",
      "   feature_name  importance_gain\n",
      "0   Column_133     23578.908250\n",
      "1     Column_7      5157.964912\n",
      "2    Column_54      4386.669757\n",
      "3   Column_107      4094.012172\n",
      "4   Column_129      4035.070673 \n",
      "\n",
      "top 5 most important features when trained on fold 2:\n",
      "   feature_name  importance_gain\n",
      "0   Column_133     23218.075441\n",
      "1    Column_54      4991.303372\n",
      "2   Column_107      4226.807395\n",
      "3   Column_129      4059.752514\n",
      "4     Column_7      3691.792320 \n",
      "\n",
      "top 5 most important features when trained on fold 3:\n",
      "   feature_name  importance_gain\n",
      "0   Column_133     23796.899673\n",
      "1     Column_7      4622.622978\n",
      "2    Column_54      3883.481706\n",
      "3   Column_129      3356.846980\n",
      "4   Column_128      3207.575537 \n",
      "\n",
      "top 5 most important features when trained on fold 4:\n",
      "   feature_name  importance_gain\n",
      "0   Column_133     23540.942354\n",
      "1     Column_7      4794.945172\n",
      "2    Column_54      4079.608554\n",
      "3   Column_107      3514.835752\n",
      "4   Column_129      3209.058444 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, top in enumerate(top5s):\n",
    "  print('top 5 most important features when trained on fold {}:\\n'.format(i), top[:5], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9r-9or_QNG9"
   },
   "source": [
    "Column 133, 7, 54, and 129 are among the top 5 for all folds. Column 107 is in all folds except the fourth one. Instead there exist column 128 among the top 5 for the fourth fold. There might be some correlation between column 107 and 128.\n",
    "\n",
    "### QUESTION 16:\n",
    "- Experiments with Subset of Features:\n",
    "\n",
    "• Remove the top 20 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtICUhb4L1A5",
    "outputId": "3cc7ec83-507a-4bec-e6c1-2c572fb3bade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.419967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21482\n",
      "[LightGBM] [Info] Number of data points in the train set: 723412, number of used features: 116\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.711728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21457\n",
      "[LightGBM] [Info] Number of data points in the train set: 716683, number of used features: 116\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.385986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21500\n",
      "[LightGBM] [Info] Number of data points in the train set: 719111, number of used features: 116\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.580664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21481\n",
      "[LightGBM] [Info] Number of data points in the train set: 718768, number of used features: 116\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.418254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21348\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 116\n"
     ]
    }
   ],
   "source": [
    "ndcg3_top20_removed = []\n",
    "ndcg5_top20_removed = []\n",
    "ndcg10_top20_removed = []\n",
    "# for i in range(1,2):\n",
    "for i in range(1,6):\n",
    "  path = 'drive/MyDrive/MSLR-WEB10K/Fold' + str(i) + '/'\n",
    "  X_train, y_train, qid_train, group_train, X_val, y_val, qid_val, group_val = load_one_fole(path)\n",
    "\n",
    "\n",
    "  aa = feature_importance[i-1]\n",
    "  # indices of top important features\n",
    "  top_imp_feat_ind = np.argsort(aa)[::-1]\n",
    "  # remove top 20 most important features\n",
    "  X_train = X_train[:, b[20:]]\n",
    "  X_val = X_val[:, b[20:]]\n",
    "\n",
    "  gbm = LGBMRanker(objective= 'lambdarank')\n",
    "  model = gbm.fit(X_train, y_train, group=group_train)\n",
    "\n",
    "  ndcg3_top20_removed.append(compute_ndcg_all(model, X_val, y_val, qid_val, k=3))\n",
    "  ndcg5_top20_removed.append(compute_ndcg_all(model, X_val, y_val, qid_val, k=5))\n",
    "  ndcg10_top20_removed.append(compute_ndcg_all(model, X_val, y_val, qid_val, k=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gz28oiXm3uWQ",
    "outputId": "d4f0091d-f8c7-41e8-b5f1-0fd880f71d0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's performance (nDCG3) on test set for different folds:\n",
      " [0.38497943398827433, 0.3816705515016462, 0.37646908990938893, 0.37770426538754925, 0.3851757652588439]\n",
      "model's performance (nDCG5) on test set for different folds:\n",
      " [0.3932316982095952, 0.38638787321379076, 0.3821637904664613, 0.38377045754959466, 0.39195510138328515]\n",
      "model's performance (nDCG10) on test set for different folds:\n",
      " [0.41678784963415666, 0.40744958230498046, 0.4042523025092867, 0.4072240303443009, 0.4122777004349055]\n"
     ]
    }
   ],
   "source": [
    "print ('model\\'s performance (nDCG3) on test set for different folds:\\n', ndcg3_top20_removed)\n",
    "print ('model\\'s performance (nDCG5) on test set for different folds:\\n', ndcg5_top20_removed)\n",
    "print ('model\\'s performance (nDCG10) on test set for different folds:\\n', ndcg10_top20_removed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Gz1W_pX5BKU"
   },
   "source": [
    "As shown above, the average nDCG has decreased by 18%. The results align with the expectation that the performance of the model decay when trained on the features from which top 20 most important ones are removed. However, it is expected that the model performance to deteriorate much more than this amount. The reason it has not decreased significantly is that there might be some retained features which are in correlation with the removed features. Therefore, those important features are still playing role in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssqC127s15GD"
   },
   "source": [
    "- Remove the 60 least important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9q3yd6tL1yJ",
    "outputId": "f5a3c65c-d78a-4793-fe60-1d70f1a5f213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.318756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13083\n",
      "[LightGBM] [Info] Number of data points in the train set: 723412, number of used features: 76\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.341413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13054\n",
      "[LightGBM] [Info] Number of data points in the train set: 716683, number of used features: 76\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.256982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13092\n",
      "[LightGBM] [Info] Number of data points in the train set: 719111, number of used features: 76\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.262742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13088\n",
      "[LightGBM] [Info] Number of data points in the train set: 718768, number of used features: 76\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.381829 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13062\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 76\n"
     ]
    }
   ],
   "source": [
    "ndcg3_bot60_removed = []\n",
    "ndcg5_bot60_removed = []\n",
    "ndcg10_bot60_removed = []\n",
    "# for i in range(1,2):\n",
    "for i in range(1,6):\n",
    "  path = 'drive/MyDrive/MSLR-WEB10K/Fold' + str(i) + '/'\n",
    "  X_train, y_train, qid_train, group_train, X_val, y_val, qid_val, group_val = load_one_fole(path)\n",
    "\n",
    "\n",
    "  aa = feature_importance[i-1]\n",
    "  # indices of least important features\n",
    "  top_imp_feat_ind = np.argsort(aa)\n",
    "  # remove least 60 most important features\n",
    "  X_train = X_train[:, b[60:]]\n",
    "  X_val = X_val[:, b[60:]]\n",
    "\n",
    "  gbm = LGBMRanker(objective= 'lambdarank')\n",
    "  model = gbm.fit(X_train, y_train, group=group_train)\n",
    "\n",
    "  ndcg3_bot60_removed.append(compute_ndcg_all(model, X_val, y_val, qid_val, k=3))\n",
    "  ndcg5_bot60_removed.append(compute_ndcg_all(model, X_val, y_val, qid_val, k=5))\n",
    "  ndcg10_bot60_removed.append(compute_ndcg_all(model, X_val, y_val, qid_val, k=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e3gcri1_2-Z",
    "outputId": "8c16e0ba-1597-4a99-dee7-8a598554bf34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's performance (nDCG3) on test set for different folds:\n",
      " [0.3391632995764517, 0.33524111860888645, 0.33003747324869515, 0.3316743968622034, 0.3299411772323068]\n",
      "model's performance (nDCG5) on test set for different folds:\n",
      " [0.35073224178104045, 0.34652388764153436, 0.34002518996590064, 0.3436793472736115, 0.3419468081292484]\n",
      "model's performance (nDCG10) on test set for different folds:\n",
      " [0.3837353393543075, 0.37203082030646895, 0.36646089619945754, 0.37086903541899086, 0.3707322538416216]\n"
     ]
    }
   ],
   "source": [
    "print ('model\\'s performance (nDCG3) on test set for different folds:\\n', ndcg3_bot60_removed)\n",
    "print ('model\\'s performance (nDCG5) on test set for different folds:\\n', ndcg5_bot60_removed)\n",
    "print ('model\\'s performance (nDCG10) on test set for different folds:\\n', ndcg10_bot60_removed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwmQhXl4CI4C"
   },
   "source": [
    "The expectation is that removing irrelevant or redundant features can help the model focus on more informative features, potentially leading to improved performance. By retaining only the most important features, the model might achieve better generalization on unseen data. However, this has not happened here as shown in the above results.\n",
    "\n",
    "The explanation might lie in the fact that removing features, even those considered less important, can result in a loss of information. There's a possibility that some of the supposedly less important features might still carry valuable information for the model, especially in complex datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOBCGOJH_8uk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
